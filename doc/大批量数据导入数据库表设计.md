
数据上传时，数据库表20万行数据库的替换如何设计
1， 禁用触发器、外键和索引：在替换过程中，暂时禁用数据库触发器、外键和索引，以提高替换性能。
2， 使用批量操作替换旧数据：
使用DELETE FROM table WHERE 1=1（或更具体的条件）删除旧数据。
使用LOAD DATA INFILE或INSERT INTO ... SELECT等方式批量插入新数据。

使用批量操作更新或插入差异数据：使用UPDATE或INSERT ... ON DUPLICATE KEY UPDATE等语句批量处理差异

2.2 COPY命令是PostgreSQL中用于批量导入和导出数据的最快方法。

3， 重建索引和启用触发器、外键：数据替换完成后，重建索引并启用之前禁用的触发器、外键等。


---


要开启rewriteBatchedStatements=true，使得数据集可以大批提交
当每条sql的数据量很大时，根据实际数据量的情况，考虑是减少单条sql数据量，还是增大max_allowed_packet单次允许提交的数据量，一般是前者
使用BufferReader去操作文件读取会非常快，毫秒级的处理响应



spring boot postgresql 大批量数据导入如何设计


---

mysql，讨论下面两种场景

开启事务
插入 1000 万条数据
提交事务
开启事务
插入 1000 万条数据
回滚事务
会不会导致数据库挂掉。

如何做到安全插入大批量数据进数据库
::
mysql 不清楚 oracle 6 曾经试过 没遇到任何问题 纵然插入数据途中服务器强制断电也没问题

参考这里: https://mariadb.com/kb/en/how-to-quickly-insert-data-into-mariadb/#using-big-transactions

> When doing many inserts in a row, you should wrap them with BEGIN / END to avoid doing a full transaction
> (which includes a disk sync) for every row. For example, doing a begin/end every 1000 inserts will speed up your > inserts by almost 1000 times.
> ...
> The reason why you may want to have many BEGIN/END statements instead of just one is that the former will use up less transaction log space.

然后发现了一个 [transaction log]( https://mariadb.com/kb/en/innodb-redo-log/ )，估计事务的一部分代价吧

开启事物，innodb 引擎会开始记录 redolog 和 undolog，写入的数据会记录到 buffer pool 的脏页中，当 redolog 写满或脏页占比过高会开始 flush 影响写入和查询的性能。未提交事物的 binlog 会记录在 binlog cache 中，binlog cache 写满也会刷到磁盘影响写入性能并且在等在等待真正的 binlog 记录时主从同步会暂停。undolog 写入同理。因为 mvcc 的机制，这个事物开始前的视图不会删除，在事物提交前会大量占用磁盘（ MySQL 5.5 及以前的版本提交后也会占用）。回滚事物，读取 undolog，删除插入的记录。未提交时异常断电会从已保存在磁盘上的 redolog 开始恢复数据。所以只要磁盘足够大，大事物的只会影响读写性能，不会让整个服务挂掉。


---

https://www.xiaolincoding.com/mysql/log/how_update.html#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-redo-log


---

1、先删除表中索引；
2、再删除需要删除的数据；
3、然后重新创建索引。


1、先选择不需要删除的数据，并把它们存在一张相同结构的空表里； 1）选择不需要删除的数据，并把它们存在一张相同结构的空表里
INSERT INTO t_copy SELECT * FROM t WHERE ... ;
2、再重命名原始表，并给新表命名为原始表的原始表名； 2）利用rename原子操作，重命名原始表和复制表
RENAME TABLE t TO t_old, t_copy TO t;
3、然后删掉原始表。3)删掉原始表
DROP TABLE t_old;


加limit的的优点：
DELETE FROM tab_name WHERE status=1 ORDER BY status LIMIT 10000;

降低写错SQL的代价，就算删错了，比如limit 500,那也就丢了500条数据，并不致命，通过binlog也可以很快恢复数据。
避免了长事务，delete执行时MySQL会将所有涉及的行加写锁和Gap锁（间隙锁），所有DML语句执行相关行会被锁住，如果删除数量大，会直接影响相关业务无法使用。
delete数据量大时，不加limit容易把cpu打满，导致越删越慢
————————————————

表分区，直接删除过期日期所在的分区（最终方案—秒杀）
ALTER TABLE table_name PARTITION BY HASH(TO_DAYS(cnt_date)) PARTITIONS 7;


---
总结
使用insert into tablA select * from tableB语句时，一定要确保tableB后面的where，order或者其他条件，都需要有对应的索引，来避免出现tableB全部记录被锁定的情况。
:
通过观察迁移sql的执行情况你会发现order_today是全表扫描，也就意味着在执行insert into select from 语句时，mysql会从上到下扫描order_today内的记录并且加锁，这样一来不就和直接锁表是一样了。

这也就可以解释，为什么一开始只有少量用户出现支付失败，后续大量用户出现支付失败，初始化订单失败等情况，因为一开始只锁定了少部分数据，没有被锁定的数据还是可以正常被修改为正常状态。由于锁定的数据越来越多，就导致出现了大量支付失败。最后全部锁住，导致无法插入订单，而出现初始化订单失败。

解决方案
由于查询条件会导致order_today全表扫描，什么能避免全表扫描呢，很简单嘛，给pay_success_time字段添加一个idx_pay_suc_time索引就可以了，由于走索引查询，就不会出现扫描全表的情况而锁表了，只会锁定符合条件的记录。

最终的sql
代码语言：txt
复制
INSERT INTO order_record SELECT
*
FROM
order_today FORCE INDEX (idx_pay_suc_time)
WHERE
pay_success_time <= '2020-03-08 00:00:00';


